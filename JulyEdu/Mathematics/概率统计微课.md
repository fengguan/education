
第1课 概率论基础
知识点1：
- 概率论基础：简要定义概率空间，事件以及事件的概率
- 常见概率公式：全概率公式，贝叶斯公式
- 随机变量：随机变量的定义，随机变量的独立性，相关性，方差，协方差
- 举例：常见分布(离散：两点分布，二项分布，连续：高斯分布，泊松分布)，并计算其中一些的期望方差
- 实战项目：朴素贝叶斯分类(上)，时间应该不多了，简要说明朴素贝叶斯方法的思想，讲需要参数估计的部分留在下一讲。

第2课 参数估计：从概率到统计
（本讲涉及到收敛性，测度论等知识，无法严格讲述，所涉及到的证明都可以是说明，力求学生掌握核心思想就可以了）
- 知识点1：分布的特征函数，并说明特征函数与矩的关系以及特征函数与分布之间的一一对应，大数定理，并说明大数定理是连接概率与统计的桥梁、中心极限定理，说明中心极限定理是比大数定理更加细致的刻画
- 举例：计算某个分布的特征函数，并利用特征函数对大数定理和中心极限定理进行简单说明。
- 知识点2：矩估计，极大似然估计
- 举例：利用矩估计和极大似然估计的方法处理高斯分布的参数估计问题
- 实战项目：朴素贝叶斯分类(下)，完成一个简单的朴素贝叶斯分类演示(利用参数估计进行模型训练)，如有时间的话加入逻辑回归作为极大似然估计的例子

第3课 参数估计的渐进性质
- 知识点1：介绍参数估计的评价指标：相合性，无偏性，渐进正态性，有效性
- 知识点2：证明极大似然估计的泛函不变性与渐进正态性，如果有时间再证明其有效性达到最优（Cramer–Rao）
- 证明可以参考：https://www.jst.go.jp/crest/math/ja/suugakujuku/archive/text/4_Miura2.pdf 从第159 到 161 页
- 实战项目：区间估计，线性回归中统计量的含义，并指出进行参数估计除了看参数的估计值外，还一定要考察参数估计值的显著性

第4课 概率统计在机器学习中的应用(唐老师的PPT非常好)
- 知识点0：凸集合与凸函数
- 知识点1：EM算法原理分析
- 知识点2：EM算法用于高斯混合模型的参数估计
- 知识点3：EM算法用于缺失值的处理

第5课：信息论简介：熵与相对熵
- 知识点1：定义熵：度量信息的方式
- 举例：正态分布为给定方差情况下熵最大的分布，并引出最大熵原理的思想
- 知识点2：相对熵，度量两个分布的差别
- 举例：相对熵与极大似然估计的关系
- 实战项目:t-sne 聚类可视化算法 http://scikit-learn.org/stable/modules/manifold.html#t-distributed-stochastic-neighbor-embedding-t-sne

第6课：最大熵原理与最大熵模型
- 知识点1：回顾最大熵原理，并利用最大熵原理建立最大熵分类模型
- 参考：https://blog.csdn.net/itplus/article/details/26549871 以及 https://en.wikipedia.org/wiki/Principle_of_maximum_entropy#Maximum_entropy_models
- 知识点2：最大熵模型的训练，此处引入最优化以及对偶问题，因为第四课讲过凸函数，这里可以大概介绍一下，然后引导学生到凸优化课程里面进一步学习
- 实战：自然语言处理实例 代码可以参考： https://blog.csdn.net/openSUSE1995/article/details/77835782
